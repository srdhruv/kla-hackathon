{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#56_Dhruv_Rajendra_Sawarkar_IITDM\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_data = {}\n",
    "example_fileName = \"Milestone3/Milestone1_Example.yaml\"\n",
    "fileA = \"Milestone3/Milestone3A.yaml\"\n",
    "fileB = \"Milestone3/Milestone3B.yaml\"\n",
    "logFileName = \"Milestone3/MileStone3A_Log.txt\"\n",
    "example = 0\n",
    "filename = fileA\n",
    "dataset_folder = \"Milestone3/\"\n",
    "\n",
    "if example == 1:\n",
    "    filename = example_fileName\n",
    "with open(filename) as file:\n",
    "    yaml_data = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "global root_logs\n",
    "root_logs = \"\"\n",
    "global history_done\n",
    "history_done = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating log file\n",
    "with open(logFileName,\"w\") as file:\n",
    "    file.write(\"\")\n",
    "def writeLogInFile(log):\n",
    "    with open(logFileName,\"a\") as file:\n",
    "        file.write(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLog(name,status,optional=\"\"):\n",
    "    #date time;name status optional\n",
    "    log = \"\"\n",
    "    now = datetime.now() #2022-03-07 10:00:07.000000;\n",
    "    neededFormat = now.strftime(\"%Y-%m-%d %H:%M:%S.%f;\")\n",
    "    log += neededFormat\n",
    "    log += name\n",
    "    log += status\n",
    "    log += optional\n",
    "    log += \"\\n\"\n",
    "    writeLogInFile(log)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(filename):\n",
    "    loc = dataset_folder + filename\n",
    "    df = pd.read_csv(loc)\n",
    "    noOfDefects = len(df)\n",
    "    return df,noOfDefects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkInputExists(vname):\n",
    "    if vname[0] != '$':\n",
    "        return 1\n",
    "    vname = vname[2:-1]\n",
    "    print(\"searching key \",vname)\n",
    "    if vname in all_data:\n",
    "        return 1\n",
    "    while vname not in all_data:\n",
    "        ;\n",
    "#         time.sleep(1)\n",
    "        \n",
    "    return 1\n",
    "\n",
    "def waitForInputs(task):\n",
    "    for k,inp in task['Inputs'].items():\n",
    "        checkInputExists(inp)\n",
    "    \n",
    "def checkCondition(cond):\n",
    "    string = cond[2:]\n",
    "    lst = string.split(' ')\n",
    "    variable = lst[0][:-1]\n",
    "    operator = lst[1]\n",
    "    value = int(lst[2])\n",
    "#     print(variable,operator,value)\n",
    "    if variable not in all_data:\n",
    "        return -1\n",
    "    if operator == '>' and all_data[variable] > value:\n",
    "        return 1\n",
    "    elif operator == '<' and all_data[variable] < value:\n",
    "        return 1\n",
    "    elif operator == '>=' and all_data[variable] >= value:\n",
    "        return 1\n",
    "    elif operator == '<=' and all_data[variable] <= value:\n",
    "        return 1\n",
    "    elif operator == '==' and all_data[variabe] == value:\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "            \n",
    "def performBinning(inputs_dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    501,Signal > 59 and Signal < 100\n",
    "    0          1  2  3   4      5 6\n",
    "    BIN_ID,RULE\n",
    "    500,Signal < 60\n",
    "    Signal > 59 and Signal < 100\n",
    "    0      1  2  3   4      5  6\n",
    "    \n",
    "    \"\"\"\n",
    "    ruleFileName = dataset_folder +  inputs_dict['RuleFilename']\n",
    "    dataset = copy.copy(all_data[inputs_dict['DataSet'][2:-1]])\n",
    "    lines = []\n",
    "    with open(ruleFileName) as rfile:\n",
    "        for line in rfile:\n",
    "            lines.append(line)\n",
    "    \n",
    "    temp = lines[1].split(',')\n",
    "    binId = temp[0]\n",
    "    temp = temp[1]\n",
    "    temp = temp.split(\" \")\n",
    "    operator1 = '>'\n",
    "    operator2 = '>'\n",
    "    val1 = 1\n",
    "    val2 = 1\n",
    "#     print(temp,len(temp))]\n",
    "        \n",
    "    operator1 = temp[1]\n",
    "    operator2 = temp[1]\n",
    "    val1 = int(temp[2])\n",
    "    val2 = int(temp[2])\n",
    "    print(\"setting val1 as \",val1)\n",
    "    \n",
    "    if len(temp) > 3:\n",
    "        operator2 = temp[5]\n",
    "        val2 = int(temp[6])\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    newCol = []\n",
    "    \n",
    "    for entry in dataset['Signal']:\n",
    "        flag1 = False\n",
    "        flag2 = False\n",
    "        if operator1 == '>' and entry > val1:\n",
    "#             print(entry,\">\",val1,\" what\")\n",
    "            flag1 = True\n",
    "        elif operator1 == '<' and entry < val1:\n",
    "            flag1 = True\n",
    "            \n",
    "        if operator2 == '>' and entry > val2:\n",
    "            flag2 = True\n",
    "        elif operator2 == '<' and entry < val2:\n",
    "            flag2 = True\n",
    "            \n",
    "        if flag1 and flag2:\n",
    "            newCol.append(binId)\n",
    "        else:\n",
    "            newCol.append(\"\")\n",
    "    dataset[\"Bincode\"] = newCol\n",
    "    newDf = dataset\n",
    "#     print(\"\\n\\nthis is new Df,\",newDf)\n",
    "    return newDf,len(newDf)\n",
    "\n",
    "def performMerge(inputs):\n",
    "    precedenceFile_loc = dataset_folder + inputs['PrecedenceFile']\n",
    "    plist = []\n",
    "    with open(precedenceFile_loc) as pfile:\n",
    "        for line in pfile:\n",
    "            plist.append(line)\n",
    "    plist = plist[0]\n",
    "    plist = plist.split(' >> ')\n",
    "    print(plist)\n",
    "    priority = {}\n",
    "    i = 0\n",
    "    for val in plist:\n",
    "        priority[val] = i\n",
    "        i += 1\n",
    "    datasets = []\n",
    "    i = -1\n",
    "    for k,v in inputs.items():\n",
    "        i = i + 1\n",
    "        if i == 0:\n",
    "            continue\n",
    "        key_d = v[2:-1]\n",
    "        datasets.append(all_data[key_d])\n",
    "        \n",
    "    finaldf = datasets[0]\n",
    "    selection = [[] for i in range(len(datasets[0]))]\n",
    "    for DataSet in datasets:\n",
    "        i = 0\n",
    "        for entry in DataSet['Bincode']:\n",
    "            if entry != '':\n",
    "                selection[i].append(entry)\n",
    "            i += 1\n",
    "    \n",
    "    newCol = []\n",
    "    \n",
    "    for lst in selection:\n",
    "        flag = 0\n",
    "        for higher in plist:\n",
    "            for option in lst:\n",
    "                if int(option) == int(higher):\n",
    "                    newCol.append(int(option))\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                break\n",
    "        if flag == 0:\n",
    "            newCol.append(0)\n",
    "    finaldf['Bincode'] = newCol\n",
    "    print(finaldf)\n",
    "    return finaldf,len(finaldf)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createResults(inputs):\n",
    "    fileName = inputs['FileName']\n",
    "    defectTableKey = inputs['DefectTable'][2:-1]\n",
    "    defectTable = all_data[defectTableKey]\n",
    "    defectTable.to_csv(dataset_folder+fileName,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkCondition(\"$(M2B_Workflow.TaskA.NoOfDefects) > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTask(name,task):\n",
    "    if task['Function'] == \"TimeFunction\":\n",
    "        \n",
    "        \n",
    "        if 'Condition' in task:\n",
    "            returnVal = checkCondition(task['Condition'])\n",
    "            if  returnVal == 0:\n",
    "                addLog(name,\" Skipped\")\n",
    "                return\n",
    "            elif returnVal == -1:\n",
    "                print(\"\\n\\nKey does not exist Yet!!!\")\n",
    "                while(1):\n",
    "                    returnVal = checkCondition(task['Condition'])\n",
    "                    if returnVal != -1:\n",
    "                        break\n",
    "                if returnVal == 0:\n",
    "                    addLog(name,\" Skipped\")\n",
    "                    return\n",
    "        waitForInputs(task)\n",
    "        \n",
    "        extra = \"TimeFunction (\"\n",
    "        values_list = list(task['Inputs'].values())\n",
    "        n = len(values_list)\n",
    "        for i in range(n):\n",
    "            if values_list[i][0] == '$':\n",
    "                extra += str(all_data[values_list[i][2:-1]])\n",
    "            else:\n",
    "                extra += values_list[i]\n",
    "            if i != n-1:\n",
    "                extra += \",\"\n",
    "        extra += ')'\n",
    "        addLog(name,\" Executing \",extra)\n",
    "        time.sleep(int(task['Inputs']['ExecutionTime']))\n",
    "        \n",
    "        \n",
    "    elif task['Function'] == 'DataLoad':\n",
    "        if 'Condition' in task:\n",
    "            returnVal = checkCondition(task['Condition'])\n",
    "            if  returnVal == 0:\n",
    "                addLog(name,\" Skipped\")\n",
    "                return\n",
    "            elif returnVal == -1:\n",
    "                print(\"\\n\\nKey does not exist Yet!!!\")\n",
    "                while(1):\n",
    "                    returnVal = checkCondition(task['Condition'])\n",
    "                    if returnVal != -1:\n",
    "                        break\n",
    "                if returnVal == 0:\n",
    "                    addLog(name,\" Skipped\")\n",
    "                    return\n",
    "        waitForInputs(task)\n",
    "        extra = \"DataLoad (\"\n",
    "        values_list = list(task['Inputs'].values())\n",
    "        n = len(values_list)\n",
    "        for i in range(n):\n",
    "            extra += values_list[i]\n",
    "            if i != n-1:\n",
    "                extra += \",\"\n",
    "        extra += ')'\n",
    "        addLog(name,\" Executing \",extra)\n",
    "        \n",
    "        \n",
    "        data_key = name + '.DataTable'\n",
    "        defect_key = name + '.NoOfDefects'\n",
    "        all_data[data_key],all_data[defect_key] = readCsv(task['Inputs']['Filename'])\n",
    "        print(\"Added key\",data_key)\n",
    "        print(\"Added key\",defect_key)\n",
    "        \n",
    "    elif task['Function'] == 'Binning':\n",
    "        if 'Condition' in task:\n",
    "            returnVal = checkCondition(task['Condition'])\n",
    "            if  returnVal == 0:\n",
    "                addLog(name,\" Skipped\")\n",
    "                return\n",
    "            elif returnVal == -1:\n",
    "                print(\"\\n\\nKey does not exist Yet!!!\")\n",
    "                while(1):\n",
    "                    returnVal = checkCondition(task['Condition'])\n",
    "                    if returnVal != -1:\n",
    "                        break\n",
    "                if returnVal == 0:\n",
    "                    addLog(name,\" Skipped\")\n",
    "                    return\n",
    "        waitForInputs(task)\n",
    "        \n",
    "        extra = \"Binning (\"\n",
    "        values_list = list(task['Inputs'].values())\n",
    "        n = len(values_list)\n",
    "        for i in range(n):\n",
    "            if values_list[i][0] == '$':\n",
    "#                 extra += str(all_data[values_list[i][2:-1]])\n",
    "#             else:\n",
    "                extra += values_list[i]\n",
    "            if i != n-1:\n",
    "                extra += \",\"\n",
    "        extra += ')'\n",
    "        addLog(name,\" Executing \",extra)\n",
    "        \n",
    "                \n",
    "        data_key = name + \".BinningResultsTable\"\n",
    "        defect_key = name + \".NoOfDefects\"\n",
    "        all_data[data_key],all_data[defect_key] = performBinning(task['Inputs'])\n",
    "        print(\"Added key\",data_key)\n",
    "        print(\"Added key\",defect_key)\n",
    "    elif task['Function'] == \"MergeResults\":\n",
    "        if 'Condition' in task:\n",
    "            returnVal = checkCondition(task['Condition'])\n",
    "            if  returnVal == 0:\n",
    "                addLog(name,\" Skipped\")\n",
    "                return\n",
    "            elif returnVal == -1:\n",
    "                print(\"\\n\\nKey does not exist Yet!!!\")\n",
    "                while(1):\n",
    "                    returnVal = checkCondition(task['Condition'])\n",
    "                    if returnVal != -1:\n",
    "                        break\n",
    "                if returnVal == 0:\n",
    "                    addLog(name,\" Skipped\")\n",
    "                    return\n",
    "        waitForInputs(task)\n",
    "        \n",
    "        data_key = name + \".MergedResults\"\n",
    "        defect_key = name + \".NoOfDefects\"\n",
    "        all_data[data_key],all_data[defect_key] = performMerge(task['Inputs'])\n",
    "        \n",
    "    elif task['Function'] == 'ExportResults':\n",
    "        if 'Condition' in task:\n",
    "            returnVal = checkCondition(task['Condition'])\n",
    "            if  returnVal == 0:\n",
    "                addLog(name,\" Skipped\")\n",
    "                return\n",
    "            elif returnVal == -1:\n",
    "                print(\"\\n\\nKey does not exist Yet!!!\")\n",
    "                while(1):\n",
    "                    returnVal = checkCondition(task['Condition'])\n",
    "                    if returnVal != -1:\n",
    "                        break\n",
    "                if returnVal == 0:\n",
    "                    addLog(name,\" Skipped\")\n",
    "                    return\n",
    "        waitForInputs(task)\n",
    "        \n",
    "        createResults(task['Inputs'])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTasksMod(name,task):\n",
    "    if task['Type'] == 'Flow':\n",
    "        addLog(name,\" Entry\")\n",
    "        #do it\n",
    "        if task['Execution'] == 'Sequential':\n",
    "            for subtaskName,subtask in task['Activities'].items():\n",
    "                doTasksMod(name+\".\"+subtaskName,subtask)\n",
    "        elif task['Execution'] == 'Concurrent':\n",
    "            # do parallel task\n",
    "            subtasks = []\n",
    "            threadList = []\n",
    "            for subtaskName,subtask in task['Activities'].items():\n",
    "                t = threading.Thread(target = doTasksMod,args=(name+\".\"+subtaskName,subtask))\n",
    "                threadList.append(t)\n",
    "            for t in threadList:\n",
    "                t.start()\n",
    "            for t in threadList:\n",
    "                t.join()\n",
    "        else:\n",
    "            print(\"\\n\\n______not serial nor paralle __________\\n\\n\")\n",
    "        addLog(name,\" Exit\")\n",
    "    elif task['Type'] == 'Task':\n",
    "        # no need to go down just do it\n",
    "        addLog(name,\" Entry\")\n",
    "        history_done[name] = \"notDone\"\n",
    "        doTask(name,task)\n",
    "        addLog(name,\" Exit\")\n",
    "        history_done[name] = \"done\"\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added key M3A_Workflow.LoadData.DataTable\n",
      "Added key M3A_Workflow.LoadData.NoOfDefects\n",
      "searching key  M3A_Workflow.LoadData.DataTable\n",
      "searching key  M3A_Workflow.LoadData.DataTable\n",
      "searching key  M3A_Workflow.LoadData.DataTable\n",
      "searching key  M3A_Workflow.LoadData.DataTable\n",
      "searching key  M3A_Workflow.LoadData.DataTable\n",
      "setting val1 as  60\n",
      "setting val1 as  59\n",
      "setting val1 as Added key M3A_Workflow.BinningProcess.BinningFor501.BinningResultsTable\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor501.NoOfDefects\n",
      "Added key 199\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor504.BinningResultsTable\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor504.NoOfDefects\n",
      " M3A_Workflow.BinningProcess.BinningFor500.BinningResultsTable\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor500.NoOfDefects\n",
      "setting val1 as  129\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor503.BinningResultsTable\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor503.NoOfDefects\n",
      "setting val1 as  99\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor502.BinningResultsTable\n",
      "Added key M3A_Workflow.BinningProcess.BinningFor502.NoOfDefects\n",
      "searching key  M3A_Workflow.BinningProcess.BinningFor500.BinningResultsTable\n",
      "searching key  M3A_Workflow.BinningProcess.BinningFor501.BinningResultsTable\n",
      "searching key  M3A_Workflow.BinningProcess.BinningFor502.BinningResultsTable\n",
      "searching key  M3A_Workflow.BinningProcess.BinningFor503.BinningResultsTable\n",
      "searching key  M3A_Workflow.BinningProcess.BinningFor504.BinningResultsTable\n",
      "['504', '503', '502', '501', '500']\n",
      "        Id    X    Y  Signal  Bincode\n",
      "0        1  212  149     243      504\n",
      "1        2  212  202      20      500\n",
      "2        3  212  220      20      500\n",
      "3        4  212  245      20      500\n",
      "4        5  213 -293      83      501\n",
      "...    ...  ...  ...     ...      ...\n",
      "5230  5231  852   53     116      502\n",
      "5231  5232  852  109     116      502\n",
      "5232  5233  852  117      74      501\n",
      "5233  5234  852  155      20      500\n",
      "5234  5235  852  181     100      502\n",
      "\n",
      "[5235 rows x 5 columns]\n",
      "searching key  M3A_Workflow.MergeBinningResults.MergedResults\n"
     ]
    }
   ],
   "source": [
    "for k,v in yaml_data.items():\n",
    "    doTasksMod(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-08 16:07:50.869204;M3A_Workflow Entry\n",
      "\n",
      "2022-03-08 16:07:50.896234;M3A_Workflow.LoadData Entry\n",
      "\n",
      "2022-03-08 16:07:50.896774;M3A_Workflow.LoadData Executing DataLoad (Milestone3A_DataInput1.csv)\n",
      "\n",
      "2022-03-08 16:07:50.904526;M3A_Workflow.LoadData Exit\n",
      "\n",
      "2022-03-08 16:07:50.905784;M3A_Workflow.BinningProcess Entry\n",
      "\n",
      "2022-03-08 16:07:50.907862;M3A_Workflow.BinningProcess.BinningFor500 Entry\n",
      "\n",
      "2022-03-08 16:07:50.908737;M3A_Workflow.BinningProcess.BinningFor500 Executing Binning (,$(M3A_Workflow.LoadData.DataTable))\n",
      "\n",
      "2022-03-08 16:07:50.909084;M3A_Workflow.BinningProcess.BinningFor502 Entry\n",
      "\n",
      "2022-03-08 16:07:50.908595;M3A_Workflow.BinningProcess.BinningFor501 Entry\n",
      "\n",
      "2022-03-08 16:07:50.910667;M3A_Workflow.BinningProcess.BinningFor504 Entry\n",
      "\n",
      "2022-03-08 16:07:50.909442;M3A_Workflow.BinningProcess.BinningFor503 Entry\n",
      "\n",
      "2022-03-08 16:07:50.911514;M3A_Workflow.BinningProcess.BinningFor501 Executing Binning (,$(M3A_Workflow.LoadData.DataTable))\n",
      "\n",
      "2022-03-08 16:07:50.913223;M3A_Workflow.BinningProcess.BinningFor503 Executing Binning (,$(M3A_Workflow.LoadData.DataTable))\n",
      "\n",
      "2022-03-08 16:07:50.914174;M3A_Workflow.BinningProcess.BinningFor504 Executing Binning (,$(M3A_Workflow.LoadData.DataTable))\n",
      "\n",
      "2022-03-08 16:07:50.913609;M3A_Workflow.BinningProcess.BinningFor502 Executing Binning (,$(M3A_Workflow.LoadData.DataTable))\n",
      "\n",
      "2022-03-08 16:07:50.930503;M3A_Workflow.BinningProcess.BinningFor500 Exit\n",
      "\n",
      "2022-03-08 16:07:50.930287;M3A_Workflow.BinningProcess.BinningFor504 Exit\n",
      "\n",
      "2022-03-08 16:07:50.926379;M3A_Workflow.BinningProcess.BinningFor501 Exit\n",
      "\n",
      "2022-03-08 16:07:50.936050;M3A_Workflow.BinningProcess.BinningFor503 Exit\n",
      "\n",
      "2022-03-08 16:07:50.946316;M3A_Workflow.BinningProcess.BinningFor502 Exit\n",
      "\n",
      "2022-03-08 16:07:50.947219;M3A_Workflow.BinningProcess Exit\n",
      "\n",
      "2022-03-08 16:07:50.947384;M3A_Workflow.MergeBinningResults Entry\n",
      "\n",
      "2022-03-08 16:07:50.977700;M3A_Workflow.MergeBinningResults Exit\n",
      "\n",
      "2022-03-08 16:07:50.977973;M3A_Workflow.ExportResult Entry\n",
      "\n",
      "2022-03-08 16:07:50.998899;M3A_Workflow.ExportResult Exit\n",
      "\n",
      "2022-03-08 16:07:50.999094;M3A_Workflow Exit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(logFileName) as file:\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M3A_Workflow.BinningProcess.BinningFor500.BinningResultsTable\n",
    "# M3A_Workflow.BinningProcess.BinningFor500.BinningResultsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
